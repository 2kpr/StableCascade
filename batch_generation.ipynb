{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb686e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, CLIPModel, CLIPVisionModelWithProjection\n",
    "from train_c import WurstCore\n",
    "from train_b import WurstCore as WurstCoreB\n",
    "from warp_core.utils import load_or_fail\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9649f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP WARPCORE\n",
    "config_file = 'configs/finetune_c_3b.yml'\n",
    "with open(config_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    loaded_config = yaml.safe_load(file)\n",
    "    loaded_config['use_fsdp'] = False\n",
    "    loaded_config['batch_size'] = 4\n",
    "\n",
    "warpcore = WurstCore(\n",
    "    config_dict=loaded_config,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# STAGE B\n",
    "config_file_b = 'configs/finetune_b_3b.yml'\n",
    "with open(config_file_b, \"r\", encoding=\"utf-8\") as file:\n",
    "    config_file_b = yaml.safe_load(file)\n",
    "    config_file_b['use_fsdp'] = False\n",
    "    config_file_b['batch_size'] = 4\n",
    "    \n",
    "warpcore_b = WurstCoreB(\n",
    "    config_dict=config_file_b,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3545cc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f41c499473f42a4965fc84d240e69d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModelWithProjection: ['text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.final_layer_norm.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'logit_scale', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_projection.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTROLNET READY\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2d1284e9b24e26805fc00ab21b4d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE B READY\n"
     ]
    }
   ],
   "source": [
    "# SETUP MODELS\n",
    "extras = warpcore.setup_extras_pre()\n",
    "extras.sampling_configs['cfg'] = 4\n",
    "models = warpcore.setup_models(extras)\n",
    "models.generator.bfloat16()\n",
    "print(\"CONTROLNET READY\")\n",
    "\n",
    "extras_b = warpcore_b.setup_extras_pre()\n",
    "extras_b.sampling_configs['cfg'] = 1.2\n",
    "models_b = warpcore_b.setup_models(extras_b)\n",
    "models_b.generator.bfloat16()\n",
    "print(\"STAGE B READY\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f04219",
   "metadata": {},
   "source": [
    "# SAFETY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c2444bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_file = \"captions_safety.yml\"\n",
    "with open(captions_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    selected_captions = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "61f2445b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_harm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [03:49<00:00, 45.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [05:20<00:00, 45.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "child\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [05:20<00:00, 45.74s/it]\n"
     ]
    }
   ],
   "source": [
    "images_per_query = 12\n",
    "batch_size = 4\n",
    "parent_dir = \"safety_images\"\n",
    "Path(parent_dir).mkdir(parents=True, exist_ok=True)\n",
    "for category in selected_captions[2:]:\n",
    "    k = list(category.keys())[0]\n",
    "    category_captions = category[k]\n",
    "    print(k)\n",
    "    \n",
    "    Path(f\"{parent_dir}/{k}\").mkdir(parents=True, exist_ok=True)\n",
    "    for caption in tqdm(category_captions):\n",
    "        if caption is None:\n",
    "            continue\n",
    "        caption_save = caption.replace(\" \", \"_\")\n",
    "        batch = {'captions': [caption]*batch_size, 'images': torch.zeros(batch_size, 3, 256, 256)}\n",
    "        conditions = warpcore.get_conditions(batch, models, extras, is_eval=True, is_unconditional=False, eval_image_embeds=False)\n",
    "        unconditions = warpcore.get_conditions(batch, models, extras, is_eval=True, is_unconditional=True, eval_image_embeds=False)    \n",
    "        \n",
    "        conditions_b = warpcore_b.get_conditions(batch, models_b, extras_b, is_eval=True, is_unconditional=False, eval_image_embeds=False)\n",
    "        unconditions_b = warpcore_b.get_conditions(batch, models_b, extras_b, is_eval=True, is_unconditional=True, eval_image_embeds=False)\n",
    "        \n",
    "        image_idx = 0\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16), torch.random.fork_rng():\n",
    "            torch.manual_seed(42)\n",
    "            for i in range(0, images_per_query//batch_size):\n",
    "                *_, (sampled_latents, _, _) = extras.gdf.sample(\n",
    "                    models.generator, conditions, (batch_size, 16, 24, 24), # (4, 16, 24, 24),\n",
    "                    unconditions, device=device, **extras.sampling_configs\n",
    "                )\n",
    "\n",
    "                conditions_b['effnet'] = sampled_latents\n",
    "                unconditions_b['effnet'] = torch.zeros_like(sampled_latents)\n",
    "                *_, (sampled_latents_b, _, _) = extras_b.gdf.sample(\n",
    "                    models_b.generator, conditions_b, (batch_size, 4, 256, 256), # (4, 4, 256, 256),\n",
    "                    unconditions_b, device=device, **extras_b.sampling_configs\n",
    "                )\n",
    "                sampled_images = models_b.stage_a.decode(sampled_latents_b).float()\n",
    "\n",
    "                for image in sampled_images:\n",
    "                    torchvision.utils.save_image(\n",
    "                        image.cpu().clamp(0, 1), \n",
    "                        f\"{parent_dir}/{k}/{caption_save}_{image_idx:03d}.jpg\"\n",
    "                    )\n",
    "                    image_idx += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f2f103cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm safety_images.tar.gz\n",
    "# !tar -zcvf safety_images.tar.gz safety_images/\n",
    "# !rm -rf safety_images/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bccf958",
   "metadata": {},
   "source": [
    "# HUMAN EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8d8dd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A city in 4-dimensional space-time', 'Pneumonoultramicroscopicsilicovolcanoconiosis', 'A black dog sitting on a wooden chair. A white cat with black ears is standing up with its paws on the chair.', 'a cat patting a crystal ball with the number 7 written on it in black marker', 'a barred owl peeking out from dense tree branches', 'a cat sitting on a stairway railing', 'a cat drinking a pint of beer', 'a bat landing on a baseball bat', 'a black dog sitting between a bush and a pair of green pants standing up with nobody inside them', 'a close-up of a blue dragonfly on a daffodil']\n"
     ]
    }
   ],
   "source": [
    "captions_file = \"captions_human_eval.yml\"\n",
    "with open(captions_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    selected_captions = yaml.safe_load(file)\n",
    "\n",
    "print(selected_captions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "892371ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [23:21<00:00,  5.47s/it]\n"
     ]
    }
   ],
   "source": [
    "images_per_query = 1\n",
    "batch_size = 1\n",
    "parent_dir = \"human_eval_wurstchen\"\n",
    "Path(parent_dir).mkdir(parents=True, exist_ok=True)\n",
    "image_idx = 0\n",
    "for caption in tqdm(selected_captions):\n",
    "    if caption is None:\n",
    "        continue\n",
    "    batch = {'captions': [caption]*batch_size, 'images': torch.zeros(batch_size, 3, 256, 256)}\n",
    "    conditions = warpcore.get_conditions(batch, models, extras, is_eval=True, is_unconditional=False, eval_image_embeds=False)\n",
    "    unconditions = warpcore.get_conditions(batch, models, extras, is_eval=True, is_unconditional=True, eval_image_embeds=False)    \n",
    "\n",
    "    conditions_b = warpcore_b.get_conditions(batch, models_b, extras_b, is_eval=True, is_unconditional=False, eval_image_embeds=False)\n",
    "    unconditions_b = warpcore_b.get_conditions(batch, models_b, extras_b, is_eval=True, is_unconditional=True, eval_image_embeds=False)\n",
    "    \n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16), torch.random.fork_rng():\n",
    "        torch.manual_seed(42)\n",
    "        for i in range(0, images_per_query//batch_size):\n",
    "            *_, (sampled_latents, _, _) = extras.gdf.sample(\n",
    "                models.generator, conditions, (batch_size, 16, 24, 24), # (4, 16, 24, 24),\n",
    "                unconditions, device=device, **extras.sampling_configs\n",
    "            )\n",
    "\n",
    "            conditions_b['effnet'] = sampled_latents\n",
    "            unconditions_b['effnet'] = torch.zeros_like(sampled_latents)\n",
    "            *_, (sampled_latents_b, _, _) = extras_b.gdf.sample(\n",
    "                models_b.generator, conditions_b, (batch_size, 4, 256, 256), # (4, 4, 256, 256),\n",
    "                unconditions_b, device=device, **extras_b.sampling_configs\n",
    "            )\n",
    "            sampled_images = models_b.stage_a.decode(sampled_latents_b).float()\n",
    "\n",
    "            for image in sampled_images:\n",
    "                torchvision.utils.save_image(\n",
    "                    image.cpu().clamp(0, 1), \n",
    "                    f\"{parent_dir}/{image_idx:09d}.png\"\n",
    "                )\n",
    "                image_idx += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f253c0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "rm: cannot remove 'human_eval_wurstchen.tar.gz': No such file or directory\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "human_eval_wurstchen/\n",
      "human_eval_wurstchen/000000024.png\n",
      "human_eval_wurstchen/000000125.png\n",
      "human_eval_wurstchen/000000139.png\n",
      "human_eval_wurstchen/000000182.png\n",
      "human_eval_wurstchen/000000244.png\n",
      "human_eval_wurstchen/000000043.png\n",
      "human_eval_wurstchen/000000063.png\n",
      "human_eval_wurstchen/000000088.png\n",
      "human_eval_wurstchen/000000094.png\n",
      "human_eval_wurstchen/000000015.png\n",
      "human_eval_wurstchen/000000008.png\n",
      "human_eval_wurstchen/000000129.png\n",
      "human_eval_wurstchen/000000058.png\n",
      "human_eval_wurstchen/000000028.png\n",
      "human_eval_wurstchen/000000241.png\n",
      "human_eval_wurstchen/000000013.png\n",
      "human_eval_wurstchen/000000194.png\n",
      "human_eval_wurstchen/000000104.png\n",
      "human_eval_wurstchen/000000077.png\n",
      "human_eval_wurstchen/000000208.png\n",
      "human_eval_wurstchen/000000201.png\n",
      "human_eval_wurstchen/000000213.png\n",
      "human_eval_wurstchen/000000096.png\n",
      "human_eval_wurstchen/000000089.png\n",
      "human_eval_wurstchen/000000091.png\n",
      "human_eval_wurstchen/000000237.png\n",
      "human_eval_wurstchen/000000041.png\n",
      "human_eval_wurstchen/000000221.png\n",
      "human_eval_wurstchen/000000163.png\n",
      "human_eval_wurstchen/000000247.png\n",
      "human_eval_wurstchen/000000254.png\n",
      "human_eval_wurstchen/000000065.png\n",
      "human_eval_wurstchen/000000095.png\n",
      "human_eval_wurstchen/000000191.png\n",
      "human_eval_wurstchen/000000109.png\n",
      "human_eval_wurstchen/000000031.png\n",
      "human_eval_wurstchen/000000222.png\n",
      "human_eval_wurstchen/000000045.png\n",
      "human_eval_wurstchen/000000211.png\n",
      "human_eval_wurstchen/000000113.png\n",
      "human_eval_wurstchen/000000207.png\n",
      "human_eval_wurstchen/000000200.png\n",
      "human_eval_wurstchen/000000038.png\n",
      "human_eval_wurstchen/000000027.png\n",
      "human_eval_wurstchen/000000190.png\n",
      "human_eval_wurstchen/000000229.png\n",
      "human_eval_wurstchen/000000087.png\n",
      "human_eval_wurstchen/000000180.png\n",
      "human_eval_wurstchen/000000233.png\n",
      "human_eval_wurstchen/000000235.png\n",
      "human_eval_wurstchen/000000010.png\n",
      "human_eval_wurstchen/000000127.png\n",
      "human_eval_wurstchen/000000052.png\n",
      "human_eval_wurstchen/000000034.png\n",
      "human_eval_wurstchen/000000075.png\n",
      "human_eval_wurstchen/000000145.png\n",
      "human_eval_wurstchen/000000162.png\n",
      "human_eval_wurstchen/000000166.png\n",
      "human_eval_wurstchen/000000206.png\n",
      "human_eval_wurstchen/000000198.png\n",
      "human_eval_wurstchen/000000248.png\n",
      "human_eval_wurstchen/000000018.png\n",
      "human_eval_wurstchen/000000159.png\n",
      "human_eval_wurstchen/000000057.png\n",
      "human_eval_wurstchen/000000131.png\n",
      "human_eval_wurstchen/000000188.png\n",
      "human_eval_wurstchen/000000021.png\n",
      "human_eval_wurstchen/000000121.png\n",
      "human_eval_wurstchen/000000212.png\n",
      "human_eval_wurstchen/000000154.png\n",
      "human_eval_wurstchen/000000240.png\n",
      "human_eval_wurstchen/000000175.png\n",
      "human_eval_wurstchen/000000186.png\n",
      "human_eval_wurstchen/000000168.png\n",
      "human_eval_wurstchen/000000116.png\n",
      "human_eval_wurstchen/000000181.png\n",
      "human_eval_wurstchen/000000030.png\n",
      "human_eval_wurstchen/000000001.png\n",
      "human_eval_wurstchen/000000059.png\n",
      "human_eval_wurstchen/000000165.png\n",
      "human_eval_wurstchen/000000072.png\n",
      "human_eval_wurstchen/000000184.png\n",
      "human_eval_wurstchen/000000239.png\n",
      "human_eval_wurstchen/000000197.png\n",
      "human_eval_wurstchen/000000090.png\n",
      "human_eval_wurstchen/000000049.png\n",
      "human_eval_wurstchen/000000005.png\n",
      "human_eval_wurstchen/000000155.png\n",
      "human_eval_wurstchen/000000232.png\n",
      "human_eval_wurstchen/000000187.png\n",
      "human_eval_wurstchen/000000108.png\n",
      "human_eval_wurstchen/000000061.png\n",
      "human_eval_wurstchen/000000020.png\n",
      "human_eval_wurstchen/000000085.png\n",
      "human_eval_wurstchen/000000255.png\n",
      "human_eval_wurstchen/000000066.png\n",
      "human_eval_wurstchen/000000118.png\n",
      "human_eval_wurstchen/000000172.png\n",
      "human_eval_wurstchen/000000228.png\n",
      "human_eval_wurstchen/000000000.png\n",
      "human_eval_wurstchen/000000179.png\n",
      "human_eval_wurstchen/000000017.png\n",
      "human_eval_wurstchen/000000101.png\n",
      "human_eval_wurstchen/000000216.png\n",
      "human_eval_wurstchen/000000026.png\n",
      "human_eval_wurstchen/000000011.png\n",
      "human_eval_wurstchen/000000093.png\n",
      "human_eval_wurstchen/000000230.png\n",
      "human_eval_wurstchen/000000144.png\n",
      "human_eval_wurstchen/000000064.png\n",
      "human_eval_wurstchen/000000250.png\n",
      "human_eval_wurstchen/000000137.png\n",
      "human_eval_wurstchen/000000192.png\n",
      "human_eval_wurstchen/000000124.png\n",
      "human_eval_wurstchen/000000245.png\n",
      "human_eval_wurstchen/000000100.png\n",
      "human_eval_wurstchen/000000226.png\n",
      "human_eval_wurstchen/000000178.png\n",
      "human_eval_wurstchen/000000196.png\n",
      "human_eval_wurstchen/000000152.png\n",
      "human_eval_wurstchen/000000156.png\n",
      "human_eval_wurstchen/000000252.png\n",
      "human_eval_wurstchen/000000033.png\n",
      "human_eval_wurstchen/000000251.png\n",
      "human_eval_wurstchen/000000111.png\n",
      "human_eval_wurstchen/000000189.png\n",
      "human_eval_wurstchen/000000081.png\n",
      "human_eval_wurstchen/000000140.png\n",
      "human_eval_wurstchen/000000002.png\n",
      "human_eval_wurstchen/000000242.png\n",
      "human_eval_wurstchen/000000023.png\n",
      "human_eval_wurstchen/000000150.png\n",
      "human_eval_wurstchen/000000202.png\n",
      "human_eval_wurstchen/000000092.png\n",
      "human_eval_wurstchen/000000185.png\n",
      "human_eval_wurstchen/000000219.png\n",
      "human_eval_wurstchen/000000231.png\n",
      "human_eval_wurstchen/000000135.png\n",
      "human_eval_wurstchen/000000039.png\n",
      "human_eval_wurstchen/000000236.png\n",
      "human_eval_wurstchen/000000112.png\n",
      "human_eval_wurstchen/000000119.png\n",
      "human_eval_wurstchen/000000134.png\n",
      "human_eval_wurstchen/000000060.png\n",
      "human_eval_wurstchen/000000098.png\n",
      "human_eval_wurstchen/000000014.png\n",
      "human_eval_wurstchen/000000205.png\n",
      "human_eval_wurstchen/000000170.png\n",
      "human_eval_wurstchen/000000056.png\n",
      "human_eval_wurstchen/000000003.png\n",
      "human_eval_wurstchen/000000040.png\n",
      "human_eval_wurstchen/000000227.png\n",
      "human_eval_wurstchen/000000183.png\n",
      "human_eval_wurstchen/000000076.png\n",
      "human_eval_wurstchen/000000025.png\n",
      "human_eval_wurstchen/000000050.png\n",
      "human_eval_wurstchen/000000132.png\n",
      "human_eval_wurstchen/000000036.png\n",
      "human_eval_wurstchen/000000171.png\n",
      "human_eval_wurstchen/000000120.png\n",
      "human_eval_wurstchen/000000029.png\n",
      "human_eval_wurstchen/000000122.png\n",
      "human_eval_wurstchen/000000128.png\n",
      "human_eval_wurstchen/000000223.png\n",
      "human_eval_wurstchen/000000199.png\n",
      "human_eval_wurstchen/000000138.png\n",
      "human_eval_wurstchen/000000070.png\n",
      "human_eval_wurstchen/000000167.png\n",
      "human_eval_wurstchen/000000142.png\n",
      "human_eval_wurstchen/000000217.png\n",
      "human_eval_wurstchen/000000195.png\n",
      "human_eval_wurstchen/000000035.png\n",
      "human_eval_wurstchen/000000051.png\n",
      "human_eval_wurstchen/000000234.png\n",
      "human_eval_wurstchen/000000220.png\n",
      "human_eval_wurstchen/000000246.png\n",
      "human_eval_wurstchen/000000078.png\n",
      "human_eval_wurstchen/000000016.png\n",
      "human_eval_wurstchen/000000107.png\n",
      "human_eval_wurstchen/000000193.png\n",
      "human_eval_wurstchen/000000117.png\n",
      "human_eval_wurstchen/000000054.png\n",
      "human_eval_wurstchen/000000149.png\n",
      "human_eval_wurstchen/000000042.png\n",
      "human_eval_wurstchen/000000164.png\n",
      "human_eval_wurstchen/000000067.png\n",
      "human_eval_wurstchen/000000114.png\n",
      "human_eval_wurstchen/000000047.png\n",
      "human_eval_wurstchen/000000068.png\n",
      "human_eval_wurstchen/000000032.png\n",
      "human_eval_wurstchen/000000105.png\n",
      "human_eval_wurstchen/000000153.png\n",
      "human_eval_wurstchen/000000173.png\n",
      "human_eval_wurstchen/000000161.png\n",
      "human_eval_wurstchen/000000053.png\n",
      "human_eval_wurstchen/000000204.png\n",
      "human_eval_wurstchen/000000209.png\n",
      "human_eval_wurstchen/000000133.png\n",
      "human_eval_wurstchen/000000080.png\n",
      "human_eval_wurstchen/000000103.png\n",
      "human_eval_wurstchen/000000210.png\n",
      "human_eval_wurstchen/000000086.png\n",
      "human_eval_wurstchen/000000006.png\n",
      "human_eval_wurstchen/000000037.png\n",
      "human_eval_wurstchen/000000169.png\n",
      "human_eval_wurstchen/000000130.png\n",
      "human_eval_wurstchen/000000225.png\n",
      "human_eval_wurstchen/000000136.png\n",
      "human_eval_wurstchen/000000079.png\n",
      "human_eval_wurstchen/000000224.png\n",
      "human_eval_wurstchen/000000218.png\n",
      "human_eval_wurstchen/000000044.png\n",
      "human_eval_wurstchen/000000055.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_eval_wurstchen/000000157.png\n",
      "human_eval_wurstchen/000000176.png\n",
      "human_eval_wurstchen/000000151.png\n",
      "human_eval_wurstchen/000000214.png\n",
      "human_eval_wurstchen/000000012.png\n",
      "human_eval_wurstchen/000000177.png\n",
      "human_eval_wurstchen/000000123.png\n",
      "human_eval_wurstchen/000000022.png\n",
      "human_eval_wurstchen/000000203.png\n",
      "human_eval_wurstchen/000000071.png\n",
      "human_eval_wurstchen/000000083.png\n",
      "human_eval_wurstchen/000000238.png\n",
      "human_eval_wurstchen/000000174.png\n",
      "human_eval_wurstchen/000000158.png\n",
      "human_eval_wurstchen/000000004.png\n",
      "human_eval_wurstchen/000000009.png\n",
      "human_eval_wurstchen/000000082.png\n",
      "human_eval_wurstchen/000000243.png\n",
      "human_eval_wurstchen/000000074.png\n",
      "human_eval_wurstchen/000000106.png\n",
      "human_eval_wurstchen/000000046.png\n",
      "human_eval_wurstchen/000000062.png\n",
      "human_eval_wurstchen/000000110.png\n",
      "human_eval_wurstchen/000000084.png\n",
      "human_eval_wurstchen/000000215.png\n",
      "human_eval_wurstchen/000000069.png\n",
      "human_eval_wurstchen/000000099.png\n",
      "human_eval_wurstchen/000000097.png\n",
      "human_eval_wurstchen/000000143.png\n",
      "human_eval_wurstchen/000000147.png\n",
      "human_eval_wurstchen/000000019.png\n",
      "human_eval_wurstchen/000000253.png\n",
      "human_eval_wurstchen/000000141.png\n",
      "human_eval_wurstchen/000000007.png\n",
      "human_eval_wurstchen/000000249.png\n",
      "human_eval_wurstchen/000000115.png\n",
      "human_eval_wurstchen/000000146.png\n",
      "human_eval_wurstchen/000000073.png\n",
      "human_eval_wurstchen/000000102.png\n",
      "human_eval_wurstchen/000000126.png\n",
      "human_eval_wurstchen/000000148.png\n",
      "human_eval_wurstchen/000000048.png\n",
      "human_eval_wurstchen/000000160.png\n"
     ]
    }
   ],
   "source": [
    "!rm human_eval_wurstchen.tar.gz\n",
    "!tar -zcvf human_eval_wurstchen.tar.gz human_eval_wurstchen/\n",
    "# !rm -rf human_eval_wurstchen/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db3ba6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
